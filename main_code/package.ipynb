{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,glob\n",
    "from urllib.parse import quote \n",
    "import requests,json,os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패스 선언\n",
    "main_datafile_path = 'data/'\n",
    "temp_files_path = 'data/temp_files/'\n",
    "main_file_name = f'{main_datafile_path}metro_ridership_by_line_stn_time.csv'\n",
    "xlsx_path = 'data/total_stn_info_20230317.xlsx'\n",
    "temp_info_name = f'{temp_files_path}temp_info.csv'\n",
    "key_path = 'data/key/kakaoapikey.txt'\n",
    "heatmap_data = f'{main_datafile_path}merged_lines.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일이름 입력받아 일일 데이터인지 전체기간 시간별 데이터인지 구볋 후 각각의 양식에 맞게 처리\n",
    "def stn_name_modification(name=main_file_name):\n",
    "    if name == main_file_name:\n",
    "        df_st = pd.read_csv(name,encoding='euc-kr')\n",
    "        df_st['지하철역'] = df_st['지하철역'].str.replace('(', ' ',regex=False,).str.split().str[0]\n",
    "        for i in df_st.index:\n",
    "            if df_st.loc[i, '지하철역'][-1] == '역':\n",
    "                df_st.loc[i, '지하철역'] = df_st.loc[i, '지하철역'][:-1]\n",
    "        return df_st\n",
    "    else:\n",
    "        df_st = pd.read_csv(name,encoding='euc-kr')\n",
    "        df_st.drop(columns=['등록일자'],inplace=True)\n",
    "        df_st.rename(columns={'역명': '지하철역'},inplace=True)\n",
    "        df_st['지하철역'] = df_st['지하철역'].str.replace('(', ' ',regex=False,).str.split().str[0]\n",
    "        for i in df_st.index:\n",
    "            if df_st.loc[i, '지하철역'][-1] == '역':\n",
    "                df_st.loc[i, '지하철역'] = df_st.loc[i, '지하철역'][:-1]\n",
    "        return df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터에 특정 조건의 열로 변경\n",
    "# 다음 단계에서 일반적으로 사용하는 호선으로 다듬기 위해서 여기서 각 호선별로 분리\n",
    "def line_sep_preproc_main():\n",
    "    # 수인분당선에서 누락되는 역명들\n",
    "    si_list = '오이도 정왕 신길오천 안산 초지 고잔 중앙 한대앞'.split()\n",
    "    copy_list = []\n",
    "    df = stn_name_modification()\n",
    "    lines = df.호선명.unique().tolist()\n",
    "    df_dict = {line: df[df['호선명'] == line].copy() for line in lines}\n",
    "    for line, frame in df_dict.items():\n",
    "        # frame = df[df['호선명']==line].copy()\n",
    "        frame['새벽 승차인원'] = frame.loc[:,['04시-05시 승차인원','05시-06시 승차인원']].sum(axis=1)\n",
    "        frame['새벽 하차인원'] = frame.loc[:,['04시-05시 하차인원','05시-06시 하차인원']].sum(axis=1)\n",
    "\n",
    "        frame['출근시간 승차인원'] = frame.loc[:,['06시-07시 승차인원','07시-08시 승차인원','08시-09시 승차인원']].sum(axis=1)\n",
    "        frame['09-16시 승차인원'] = frame.loc[:,['09시-10시 승차인원','10시-11시 승차인원','11시-12시 승차인원',\n",
    "                                                '12시-13시 승차인원','13시-14시 승차인원','14시-15시 승차인원','16시-17시 승차인원']].sum(axis=1)\n",
    "        frame['퇴근시간 승차인원'] = frame.loc[:,['17시-18시 승차인원','18시-19시 승차인원','19시-20시 승차인원']].sum(axis=1)\n",
    "        frame['야간 승차인원'] = frame.loc[:,['20시-21시 승차인원','21시-22시 승차인원','22시-23시 승차인원',\n",
    "                                            '23시-24시 승차인원','00시-01시 승차인원','01시-02시 승차인원']].sum(axis=1)\n",
    "        frame['출근시간 하차인원'] = frame.loc[:,['06시-07시 하차인원','07시-08시 하차인원','08시-09시 하차인원']].sum(axis=1)\n",
    "        frame['퇴근시간 하차인원'] = frame.loc[:,['17시-18시 하차인원','18시-19시 하차인원','19시-20시 하차인원']].sum(axis=1)\n",
    "        frame['09-16시 하차인원'] = frame.loc[:,['09시-10시 하차인원','10시-11시 하차인원','11시-12시 하차인원',\n",
    "                                                '12시-13시 하차인원','13시-14시 하차인원','14시-15시 하차인원','16시-17시 하차인원']].sum(axis=1)\n",
    "        frame['야간 하차인원'] = frame.loc[:,['20시-21시 하차인원','21시-22시 하차인원','22시-23시 하차인원',\n",
    "                                            '23시-24시 하차인원','00시-01시 하차인원','01시-02시 하차인원']].sum(axis=1)\n",
    "        frame['총 승차인원'] = frame.loc[:,['새벽 승차인원','출근시간 승차인원','09-16시 승차인원','야간 승차인원']].sum(axis=1)\n",
    "        frame['총 하차인원'] = frame.loc[:,['새벽 하차인원','출근시간 하차인원','09-16시 하차인원','야간 하차인원']].sum(axis=1)\n",
    "        frame = frame[['사용월', '호선명', '지하철역', '출근시간 승차인원', '출근시간 하차인원', \n",
    "                        '09-16시 승차인원', '09-16시 하차인원', '퇴근시간 승차인원', '퇴근시간 하차인원',\n",
    "                        '새벽 승차인원','새벽 하차인원','야간 승차인원', '야간 하차인원','총 승차인원','총 하차인원']]\n",
    "        # 2호선 신천역이 잠실새내역으로 바뀌고 새로운 신천역 생겨서 예전 자료에서의 2호선 신천역 잠실새내 역으로 변경\n",
    "        frame.loc[(frame['호선명'] == '2호선') & (frame['지하철역'] == '신천'), '지하철역'] = '잠실새내'\n",
    "        # 수인분당선에서 누락된 안산선의 역들 따로 추출\n",
    "        frame_copy = frame[(frame['호선명']=='안산선')&(frame['지하철역'].isin(si_list))].copy()\n",
    "        frame_copy['호선명'] = frame_copy['호선명'].apply(lambda x: '수인선_누락')\n",
    "        frame_copy = frame_copy.reset_index(drop=True)\n",
    "        copy_list.append(frame_copy)\n",
    "        frame.to_csv(f'{temp_files_path}{line}.csv',index=False,encoding='utf-8')\n",
    "    pd.concat(copy_list).to_csv(f'{temp_files_path}수인선_누락.csv',index=False,encoding='utf-8')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 정해둔 양식으로 호선을 분리\n",
    "# 2019 이후 자료에는 9호선 2단계가 없어서 따로 처리\n",
    "def rtn_line_info(year):\n",
    "    path = temp_files_path\n",
    "    line_info = [\n",
    "        ([f'{path}1호선.csv', f'{path}경부선.csv', f'{path}경원선.csv', f'{path}경인선.csv', f'{path}장항선.csv'], '1호선'),\n",
    "        ([f'{path}2호선.csv'], '2호선'),\n",
    "        ([f'{path}3호선.csv', f'{path}일산선.csv'], '3호선'),\n",
    "        ([f'{path}4호선.csv', f'{path}과천선.csv', f'{path}안산선.csv'], '4호선'),\n",
    "        ([f'{path}5호선.csv'], '5호선'),\n",
    "        ([f'{path}6호선.csv'], '6호선'),\n",
    "        ([f'{path}7호선.csv'], '7호선'),\n",
    "        ([f'{path}8호선.csv'], '8호선'),\n",
    "        ([f'{path}경춘선.csv'], '경춘선'),\n",
    "        ([f'{path}수인선.csv',f'{path}수인선_누락.csv', f'{path}분당선.csv'], '수인분당선'),\n",
    "        ([f'{path}경의선.csv', f'{path}중앙선.csv'], '경의중앙선'),\n",
    "        ([f'{path}공항철도 1호선.csv'], '공항철도')\n",
    "    ]\n",
    "    # 19년도 이후 자료에는 9호선 2단계가 없음\n",
    "    if year >= 2019:\n",
    "        line_info.append(([f'{path}9호선.csv', f'{path}9호선2~3단계.csv'], '9호선'))\n",
    "    else:\n",
    "        line_info.append(([f'{path}9호선.csv', f'{path}9호선2단계.csv', f'{path}9호선2~3단계.csv'], '9호선'))\n",
    "    return line_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rtn_line_info(year) 함수에서 만들어진 파일들을 제거하는 함수\n",
    "def rm_temp_files():\n",
    "    temp_path = os.path.join('data', 'temp_files', '*.csv')\n",
    "    for file in glob.glob(temp_path):\n",
    "        os.remove(file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시로 만들어진 info 파일을 제거하는 함수\n",
    "def rm_temp_info_files(year,ck_week):\n",
    "    file = os.path.join('data',  f'{year}_{ck_week}_sub_info.csv')\n",
    "    os.remove(file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 호선 전처리 함수\n",
    "# rtn_line_info() 함수에서 불러온 line_info를 이용\n",
    "# 호선명은 불러온 line_info에 있는 line_name 으로 사용\n",
    "def preprocessing_lines(year=2019):\n",
    "    line_list = []\n",
    "    line_info = rtn_line_info(year)\n",
    "    # df_copies에는 df_list에 있는 csv파일들을 불러와서 데이터프레임으로 만들고 append함\n",
    "    for df_list, line_name in line_info:\n",
    "        df_copies = []\n",
    "        for file in df_list:\n",
    "            df = pd.read_csv(file)\n",
    "            df_copies.append(df.copy())\n",
    "        # 모두 불러왔으면 concat 함수로 하나의 데이터프레임으로 만들고 인덱스 정리\n",
    "        result = pd.concat(df_copies, axis=0)\n",
    "        result = result.reset_index(drop=True)\n",
    "        result.호선명 = line_name\n",
    "        cols = list(result.columns)[:3]\n",
    "        target = list(result.columns)[3:]\n",
    "        # 그 후 groupby로 중복된 역들의 데이터 합산 ex) 1호선 서울역, 경부선 서울역\n",
    "        res = result.groupby(cols)[target].agg('sum').reset_index()\n",
    "        line_list.append(res)\n",
    "    final = pd.concat(line_list,axis=0)\n",
    "    rm_temp_files()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 데이터파일 전처리 함수\n",
    "# 함수 호출시 merged_lines.csv 파일 생성\n",
    "def preproc_main():\n",
    "    line_sep_preproc_main()\n",
    "    final = preprocessing_lines()\n",
    "    final.to_csv(f'{main_datafile_path}merged_lines.csv',index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 년도 일일 데이터 전처리 함수\n",
    "# 연도와 주중/주말 입력받아 실행\n",
    "# 각 년도의 일일 데이터를 월별 데이터로 변환\n",
    "def stn_sub_modification(year,ck_week):\n",
    "    name = f'{main_datafile_path}{year}.csv'\n",
    "    \n",
    "    df1 = stn_name_modification(name)\n",
    "    cols = list(df1.columns)[:3]\n",
    "    target = list(df1.columns)[3:]\n",
    "    df1['사용일자'] = pd.to_datetime(df1['사용일자'], format='%Y%m%d')\n",
    "    # 평일과 주말 구분하는 새로운 열 생성\n",
    "    df1['주중/주말'] = df1['사용일자'].apply(lambda x: '주말' if x.weekday() >= 5 else '주중')\n",
    "    # 주중/주말 선택\n",
    "    rtn_week = '주중' if ck_week == '주중' else '주말'\n",
    "    weekday_df = df1[df1['주중/주말'] == rtn_week]\n",
    "    week_df = weekday_df.copy()\n",
    "    week_df['사용일자'] = pd.to_datetime(weekday_df['사용일자']).dt.strftime('%Y%m%d').astype(int)\n",
    "    df_list = []\n",
    "    for i in range(1, 13):\n",
    "        start_date = year*10000 + i*100\n",
    "        end_date = start_date + 100\n",
    "        df_temp = week_df[(week_df['사용일자'] >= start_date) & (week_df['사용일자'] < end_date)].copy()\n",
    "        df_temp['사용일자'] = year*100 + i\n",
    "        df_temp = df_temp.groupby(cols)[target].agg('sum').reset_index()\n",
    "        df_list.append(df_temp)\n",
    "    df_res = pd.concat(df_list, axis=0)\n",
    "    df_res.to_csv(f'{main_datafile_path}{year}_{ck_week}_sub_info.csv', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일일 데이터 가공\n",
    "# preproc_main과 크게 다르지는 않음\n",
    "def preproc_sub(year,ck_week):\n",
    "    \n",
    "    stn_sub_modification(year,ck_week)\n",
    "    df = pd.read_csv(f'{main_datafile_path}{year}_{ck_week}_sub_info.csv')\n",
    "    df = df.rename(columns={'역명': '지하철역', '노선명':'호선명', '사용일자':'사용월'})\n",
    "    \n",
    "    lines = df.호선명.unique().tolist()\n",
    "    df_dict = {line: df[df['호선명'] == line].copy() for line in lines}\n",
    "    copy_list = []\n",
    "    si_list = '오이도 정왕 신길오천 안산 초지 고잔 중앙 한대앞'.split()\n",
    "    for line, frame in df_dict.items():\n",
    "        frame.loc[(frame['호선명'] == '2호선') & (frame['지하철역'] == '신천'), '지하철역'] = '잠실새내'\n",
    "        \n",
    "        frame_copy = frame[(frame['호선명']=='안산선')&(frame['지하철역'].isin(si_list))].copy()\n",
    "        frame_copy['호선명'] = frame_copy['호선명'].apply(lambda x: '수인선_누락')\n",
    "        frame_copy = frame_copy.reset_index(drop=True)\n",
    "        copy_list.append(frame_copy)\n",
    "        frame.to_csv(f'{temp_files_path}{line}.csv',index=False,encoding='utf-8')\n",
    "    pd.concat(copy_list).to_csv(f'{temp_files_path}수인선_누락.csv',index=False,encoding='utf-8')\n",
    "\n",
    "    final = preprocessing_lines(year)\n",
    "    rm_temp_files()\n",
    "    rm_temp_info_files(year,ck_week)\n",
    "    final.to_csv(f'{main_datafile_path}{year}_{ck_week}_merged_lines.csv',index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kakao local API로 좌표를 구하는 함수\n",
    "def kakao_location(place):\n",
    "    with open({key_path}) as f_:\n",
    "        kakao_key = f_.read()\n",
    "    base_url = \"https://dapi.kakao.com/v2/local/search/address.json\"\n",
    "    url = f'{base_url}?query={quote(place)}'\n",
    "    header = {'Authorization':f'KakaoAK {kakao_key}'}\n",
    "    result = requests.get(url, headers=header).json()\n",
    "    lat_ = float(result['documents'][0]['y'])\n",
    "    lng_ = float(result['documents'][0]['x'])\n",
    "    return lat_,lng_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임에서 역명을 입력받아 도로명주소를 반환하는 함수\n",
    "def rtn_addr(df,target):\n",
    "    str_addr = df[df.지하철역 == target].도로명주소.values[-1]\n",
    "    return str_addr.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 역사 데이터에서 사용할 부분만 추출\n",
    "# 그 역사들의 위경도를 구해서 추가\n",
    "# 오타/오기 등으로 잘못 된 역사 주소 수정\n",
    "def get_stn_lat_lng():\n",
    "    df = pd.read_excel(xlsx_path,engine='openpyxl')\n",
    "    df.to_csv(temp_info_name,index=False)\n",
    "    df = pd.read_csv(temp_info_name)\n",
    "    df = df[['역사명','역사도로명주소','운영기관명']]\n",
    "    df.rename(columns={'역사명':'지하철역','역사도로명주소':'도로명주소'},inplace=True)\n",
    "    exclude_list = ['대전교통공사', '대구도시철도공사', '부산광역시 부산교통공사', '부산-김해경전철㈜','광주광역시 도시철도공사']\n",
    "    for exclude in exclude_list:\n",
    "        df = df[df['운영기관명'] != exclude]\n",
    "    df.drop(columns=['운영기관명'],inplace=True)\n",
    "    df['지하철역'] = df['지하철역'].str.replace('(', ' ',regex=False,).str.split().str[0]\n",
    "    for i in df.index:\n",
    "        if df['지하철역'][i][-1] == '역':\n",
    "            df['지하철역'][i] = df['지하철역'][i][:-1]\n",
    "    df.drop_duplicates(subset=['지하철역'],keep='first',inplace=True)\n",
    "    df = df[~df['도로명주소'].str.contains('부산|울산')]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    temp1 =[]\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            target = df['지하철역'][i].strip()\n",
    "            temp1.append(kakao_location(rtn_addr(df,target)))\n",
    "        except:\n",
    "            print(i, df.지하철역[i])\n",
    "            \n",
    "    df_test = pd.DataFrame(temp1,columns=('lat','lng'))\n",
    "    df2 = pd.concat([df, df_test], axis=1)\n",
    "    df2.to_csv(f'{main_datafile_path}stn_r_addr_final.csv',index=False)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히트맵을 사용하기 위해 get_stn_lat_lng()에서 구한 역사별 위경도 자료와 merge함\n",
    "def add_lat_lng(name=heatmap_data):\n",
    "    df_latlng = pd.read_csv(f'{main_datafile_path}stn_r_addr_final.csv')\n",
    "    df_latlng.drop(columns=['도로명주소'], inplace=True)\n",
    "\n",
    "    df_main = pd.read_csv(name)\n",
    "    res = pd.merge(df_main, df_latlng, on='지하철역', how='left')\n",
    "    res.to_csv(f'{main_datafile_path}lines_4heatmap_{name[5:12]}.csv', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일일 데이터는 2020.csv 형식으로 'euc-kr' 인코딩 파일\n",
    "# 메인 파일 전처리\n",
    "# preproc_main()\n",
    "# 일일 데이터 전처리 // 'year' , '주중 or 주말'\n",
    "# preproc_sub(2021,'주말')\n",
    "# 히트맵 사용하기위한 처리\n",
    "# add_lat_lng()\n",
    "# 일일 데이터 히트맵 처리 // 해당 년도 파일명\n",
    "# add_lat_lng('data/2020_주중_merged_lines.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sum_columns(df, columns, new_column_name):\n",
    "#     df[new_column_name] = df.loc[:, columns].sum(axis=1)\n",
    "#     return df\n",
    "\n",
    "# def line_sep_preproc_main():\n",
    "#     # 수인분당선에서 누락되는 역명들\n",
    "#     si_list = '오이도 정왕 신길오천 안산 초지 고잔 중앙 한대앞'.split()\n",
    "#     copy_list = []\n",
    "#     df = stn_name_modification()\n",
    "#     lines = df.호선명.unique().tolist()\n",
    "#     df_dict = {line: df[df['호선명'] == line].copy() for line in lines}\n",
    "#     for line, frame in df_dict.items():\n",
    "#         frame = sum_columns(frame, ['04시-05시 승차인원','05시-06시 승차인원'], '새벽 승차인원')\n",
    "#         frame = sum_columns(frame, ['04시-05시 하차인원','05시-06시 하차인원'], '새벽 하차인원')\n",
    "#         frame = sum_columns(frame, ['06시-07시 승차인원','07시-08시 승차인원','08시-09시 승차인원'], '출근시간 승차인원')\n",
    "#         frame = sum_columns(frame, ['09시-10시 승차인원','10시-11시 승차인원','11시-12시 승차인원',\n",
    "#                                     '12시-13시 승차인원','13시-14시 승차인원','14시-15시 승차인원','15시-16시 승차인원','16시-17시 승차인원'], '09-16시 승차인원')\n",
    "#         frame = sum_columns(frame, ['17시-18시 승차인원','18시-19시 승차인원','19시-20시 승차인원'], '퇴근시간 승차인원')\n",
    "#         frame = sum_columns(frame, ['20시-21시 승차인원','21시-22시 승차인원','22시-23시 승차인원',\n",
    "#                                     '23시-24시 승차인원','00시-01시 승차인원','01시-02시 승차인원'], '야간 승차인원')\n",
    "#         frame = sum_columns(frame, ['06시-07 시 하 차 인 원 ','07 시 -08 시 하 차 인 원 ','08 시 -09 시 하 차 인 원 '], '출근 시간 하 차 인 원 ')\n",
    "#         frame = sum_columns(frame, ['17 시 -18 시 하 차 인 원 ','18 시 -19 시 하 차 인 원 ','19 시 -20 시 하 차 인 원 '], '퇴근 시간 하 차 인 원 ')\n",
    "#         frame = sum_columns(frame, ['09 시 -10 시 하 차 인 원 ','10 시 -11 시 하 차 인 원 ','11 시 -12 시 하 차 인 원 ',\n",
    "#                                     '12 시 -13 시 하 차 인 원 ','13 시 -14 시 하 차 인 원 ','14 시 -15 시 하 차 인 원 ','15 시 -16 시 하 차 인 원 ','16 시 -17 시 하 차 인 원 '], '09-16 시 하 차 인 원 ')\n",
    "#         frame = sum_columns(frame, ['20 시 -21 시 하 차 인 원 ','21 시 -22 시 하 차 인 원 ','22 시 -23 시 하 차 인 원 ',\n",
    "#                                     '23 시 -24 시 하 차 인 원 ','00 시 -01 시 하 차 인 원 ','01 시 -02 시 하 차 인 원 '], '야간 하 차 인 원 ')\n",
    "#         frame = sum_columns(frame, ['새벽 승차인원','출근시간 승차인원','09-16시 승차인원','퇴근시간 승차인원','야간 승차인원'], '총 승차인원')\n",
    "#         frame = sum_columns(frame, ['새벽 하차인원','출근시간 하차인원','09-16시 하차인원','퇴근시간 하차인원','야간 하차인원'], '총 하차인원')\n",
    "#         frame = frame[['사용월', '호선명', '지하철역', '출근시간 승차인원', '출근시간 하차인원', \n",
    "#                         '09-16시 승차인원', '09-16시 하차인원', '퇴근시간 승차인원', '퇴근시간 하차인원',\n",
    "#                         '새벽 승차인원','새벽 하차인원','야간 승차인원', '야간 하차인원','총 승차인원','총 하차인원']]\n",
    "#         # 2호선 신천역이 잠실새내역으로 바뀌고 새로운 신천역 생겨서 예전 자료에서의 2호선 신천역 잠실새내 역으로 변경\n",
    "#         frame.loc[(frame['호선명'] == '2호선') & (frame['지하철역'] == '신천'), '지하철역'] = '잠실새내'\n",
    "#         # 수인분당선에서 누락된 안산선의 역들 따로 추출\n",
    "#         frame_copy = frame[(frame['호선명']=='안산선')&(frame['지하철역'].isin(si_list))].copy()\n",
    "#         frame_copy['호선명'] = frame_copy['호선명'].apply(lambda x: '수인선_누락')\n",
    "#         frame_copy = frame_copy.reset_index(drop=True)\n",
    "#         copy_list.append(frame_copy)\n",
    "#         frame.to_csv(f'{temp_files_path}{line}.csv',index=False,encoding='utf-8')\n",
    "#     pd.concat(copy_list).to_csv(f'{temp_files_path}수인선_누락.csv',index=False,encoding='utf-8')\n",
    "#     return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
