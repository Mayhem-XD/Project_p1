{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,glob\n",
    "from urllib.parse import quote \n",
    "import requests,json,os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_datafile_path = 'data/'\n",
    "temp_files_path = 'data/temp_files/'\n",
    "main_file_name = f'{main_datafile_path}metro_ridership_by_line_stn_time.csv'\n",
    "xlsx_path = 'data/total_stn_info_20230317.xlsx'\n",
    "temp_info_name = f'{temp_files_path}temp_info.csv'\n",
    "key_path = '../data/main/key/kakaoapikey.txt'\n",
    "heatmap_data = f'{main_datafile_path}merged_lines.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력한 파일 네임이 월별/연도별에 따라 칼럼명 수정\n",
    "def stn_name_modification(name=main_file_name):\n",
    "    if name == main_file_name:\n",
    "        df_st = pd.read_csv(name,encoding='euc-kr')\n",
    "        df_st['지하철역'] = df_st['지하철역'].str.replace('(', ' ',regex=False,).str.split().str[0]\n",
    "        for i in df_st.index:\n",
    "            if df_st.loc[i, '지하철역'][-1] == '역':\n",
    "                df_st.loc[i, '지하철역'] = df_st.loc[i, '지하철역'][:-1]\n",
    "        return df_st\n",
    "    else:\n",
    "        df_st = pd.read_csv(name,encoding='euc-kr')\n",
    "        df_st.drop(columns=['등록일자'],inplace=True)\n",
    "        df_st.rename(columns={'역명': '지하철역'},inplace=True)\n",
    "        for i in df_st.index:\n",
    "            if df_st.loc[i, '지하철역'][-1] == '역':\n",
    "                df_st.loc[i, '지하철역'] = df_st.loc[i, '지하철역'][:-1]\n",
    "        return df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간별 데이터 정리\n",
    "def line_sep_preproc_main():\n",
    "    \n",
    "    df = stn_name_modification()\n",
    "    lines = df.호선명.unique().tolist()\n",
    "    df_dict = {line: df[df['호선명'] == line].copy() for line in lines}\n",
    "    for line, frame in df_dict.items():\n",
    "        # frame = df[df['호선명']==line].copy()\n",
    "        frame['새벽 승차인원'] = frame.loc[:,['04시-05시 승차인원','05시-06시 승차인원']].sum(axis=1)\n",
    "        frame['새벽 하차인원'] = frame.loc[:,['04시-05시 하차인원','05시-06시 하차인원']].sum(axis=1)\n",
    "\n",
    "        frame['출근시간 승차인원'] = frame.loc[:,['06시-07시 승차인원','07시-08시 승차인원','08시-09시 승차인원']].sum(axis=1)\n",
    "        frame['09-16시 승차인원'] = frame.loc[:,['09시-10시 승차인원','10시-11시 승차인원','11시-12시 승차인원',\n",
    "                                                '12시-13시 승차인원','13시-14시 승차인원','14시-15시 승차인원','16시-17시 승차인원']].sum(axis=1)\n",
    "        frame['퇴근시간 승차인원'] = frame.loc[:,['17시-18시 승차인원','18시-19시 승차인원','19시-20시 승차인원']].sum(axis=1)\n",
    "        frame['야간 승차인원'] = frame.loc[:,['20시-21시 승차인원','21시-22시 승차인원','22시-23시 승차인원',\n",
    "                                            '23시-24시 승차인원','00시-01시 승차인원','01시-02시 승차인원']].sum(axis=1)\n",
    "        frame['출근시간 하차인원'] = frame.loc[:,['06시-07시 하차인원','07시-08시 하차인원','08시-09시 하차인원']].sum(axis=1)\n",
    "        frame['퇴근시간 하차인원'] = frame.loc[:,['17시-18시 하차인원','18시-19시 하차인원','19시-20시 하차인원']].sum(axis=1)\n",
    "        frame['09-16시 하차인원'] = frame.loc[:,['09시-10시 하차인원','10시-11시 하차인원','11시-12시 하차인원',\n",
    "                                                '12시-13시 하차인원','13시-14시 하차인원','14시-15시 하차인원','16시-17시 하차인원']].sum(axis=1)\n",
    "        frame['야간 하차인원'] = frame.loc[:,['20시-21시 하차인원','21시-22시 하차인원','22시-23시 하차인원',\n",
    "                                            '23시-24시 하차인원','00시-01시 하차인원','01시-02시 하차인원']].sum(axis=1)\n",
    "        frame['총 승차인원'] = frame.loc[:,['새벽 승차인원','출근시간 승차인원','09-16시 승차인원','야간 승차인원']].sum(axis=1)\n",
    "        frame['총 하차인원'] = frame.loc[:,['새벽 하차인원','출근시간 하차인원','09-16시 하차인원','야간 하차인원']].sum(axis=1)\n",
    "        frame = frame[['사용월', '호선명', '지하철역', '출근시간 승차인원', '출근시간 하차인원', \n",
    "                        '09-16시 승차인원', '09-16시 하차인원', '퇴근시간 승차인원', '퇴근시간 하차인원',\n",
    "                        '새벽 승차인원','새벽 하차인원','야간 승차인원', '야간 하차인원','총 승차인원','총 하차인원']]\n",
    "        \n",
    "        frame.loc[(frame['호선명'] == '2호선') & (frame['지하철역'] == '신천'), '지하철역'] = '잠실새내'\n",
    "\n",
    "        frame.to_csv(f'{temp_files_path}{line}.csv',index=False,encoding='utf-8')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연도에 따라 (9호선 2단계 있고 없는거) 호선 정리\n",
    "def rtn_line_info(year):\n",
    "    path = temp_files_path\n",
    "    if year >= 2019:\n",
    "        line_info = [\n",
    "        ([f'{path}1호선.csv', f'{path}경부선.csv', f'{path}경원선.csv', f'{path}경인선.csv', f'{path}장항선.csv'], '1호선'),\n",
    "        ([f'{path}2호선.csv'], '2호선'),\n",
    "        ([f'{path}3호선.csv', f'{path}일산선.csv'], '3호선'),\n",
    "        ([f'{path}4호선.csv', f'{path}과천선.csv', f'{path}안산선.csv'], '4호선'),\n",
    "        ([f'{path}5호선.csv'], '5호선'),\n",
    "        ([f'{path}6호선.csv'], '6호선'),\n",
    "        ([f'{path}7호선.csv'], '7호선'),\n",
    "        ([f'{path}8호선.csv'], '8호선'),\n",
    "        ([f'{path}9호선.csv', f'{path}9호선2~3단계.csv'], '9호선'),\n",
    "        ([f'{path}수인선.csv', f'{path}분당선.csv'], '수인분당선'),\n",
    "        ([f'{path}경의선.csv', f'{path}중앙선.csv'], '경의중앙선'),\n",
    "        ([f'{path}공항철도 1호선.csv'], '공항철도')\n",
    "    ]\n",
    "    else:\n",
    "        line_info = [\n",
    "            ([f'{path}1호선.csv', f'{path}경부선.csv', f'{path}경원선.csv', f'{path}경인선.csv', f'{path}장항선.csv'], '1호선'),\n",
    "            ([f'{path}2호선.csv'], '2호선'),\n",
    "            ([f'{path}3호선.csv', f'{path}일산선.csv'], '3호선'),\n",
    "            ([f'{path}4호선.csv', f'{path}과천선.csv', f'{path}안산선.csv'], '4호선'),\n",
    "            ([f'{path}5호선.csv'], '5호선'),\n",
    "            ([f'{path}6호선.csv'], '6호선'),\n",
    "            ([f'{path}7호선.csv'], '7호선'),\n",
    "            ([f'{path}8호선.csv'], '8호선'),\n",
    "            ([f'{path}9호선.csv', f'{path}9호선2단계.csv', f'{path}9호선2~3단계.csv'], '9호선'),\n",
    "            ([f'{path}수인선.csv', f'{path}분당선.csv'], '수인분당선'),\n",
    "            ([f'{path}경의선.csv', f'{path}중앙선.csv'], '경의중앙선'),\n",
    "            ([f'{path}공항철도 1호선.csv'], '공항철도')\n",
    "        ]\n",
    "    return line_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rm_temp_files():\n",
    "    temp_path = os.path.join('data', 'temp_files', '*.csv')\n",
    "    for file in glob.glob(temp_path):\n",
    "        os.remove(file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lines(year=2019):\n",
    "    line_list = []\n",
    "    line_info = rtn_line_info(year)\n",
    "    for df_list, line_name in line_info:\n",
    "        df_copies = []\n",
    "        for file in df_list:\n",
    "            df = pd.read_csv(file)\n",
    "            df_copies.append(df.copy())\n",
    "        result = pd.concat(df_copies, axis=0)\n",
    "        result = result.reset_index(drop=True)\n",
    "        result.호선명 = line_name\n",
    "        cols = list(result.columns)[:3]\n",
    "        target = list(result.columns)[3:]\n",
    "        res = result.groupby(cols)[target].agg('sum').reset_index()\n",
    "        line_list.append(res)\n",
    "    final = pd.concat(line_list,axis=0)\n",
    "    rm_temp_files()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_main():\n",
    "    line_sep_preproc_main()\n",
    "    final = preprocessing_lines()\n",
    "    final.to_csv(f'{main_datafile_path}merged_lines.csv',index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stn_sub_modification(year,ck_week):\n",
    "    name = f'{main_datafile_path}{year}.csv'\n",
    "    \n",
    "    df1 = stn_name_modification(name)\n",
    "    cols = list(df1.columns)[:3]\n",
    "    target = list(df1.columns)[3:]\n",
    "    df1['사용일자'] = pd.to_datetime(df1['사용일자'], format='%Y%m%d')\n",
    "    # 평일과 주말 구분하는 새로운 열 생성\n",
    "    df1['주중/주말'] = df1['사용일자'].apply(lambda x: '주말' if x.weekday() >= 5 else '주중')\n",
    "    # 주중/주말 선택    나중에 에러방지 추가해야함\n",
    "    rtn_week = '주중' if ck_week == '주중' else '주말'\n",
    "    weekday_df = df1[df1['주중/주말'] == rtn_week]\n",
    "    week_df = weekday_df.copy()\n",
    "    week_df['사용일자'] = pd.to_datetime(weekday_df['사용일자']).dt.strftime('%Y%m%d').astype(int)\n",
    "    df_list = []\n",
    "    for i in range(1, 13):\n",
    "        start_date = year*10000 + i*100\n",
    "        end_date = start_date + 100\n",
    "        df_temp = week_df[(week_df['사용일자'] >= start_date) & (week_df['사용일자'] < end_date)].copy()\n",
    "        df_temp['사용일자'] = year*100 + i\n",
    "        df_temp = df_temp.groupby(cols)[target].agg('sum').reset_index()\n",
    "        df_list.append(df_temp)\n",
    "    df_res = pd.concat(df_list, axis=0)\n",
    "    df_res.to_csv(f'{main_datafile_path}{year}_{ck_week}_sub_info.csv', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_sub(year,ck_week):\n",
    "    \n",
    "    stn_sub_modification(year,ck_week)\n",
    "    df = pd.read_csv(f'{main_datafile_path}{year}_{ck_week}_sub_info.csv')\n",
    "    df = df.rename(columns={'역명': '지하철역', '노선명':'호선명', '사용일자':'사용월'})\n",
    "    \n",
    "    lines = df.호선명.unique().tolist()\n",
    "    df_dict = {line: df[df['호선명'] == line].copy() for line in lines}\n",
    "    for line, frame in df_dict.items():\n",
    "        frame.loc[(frame['호선명'] == '2호선') & (frame['지하철역'] == '신천'), '지하철역'] = '잠실새내'\n",
    "        frame.to_csv(f'{temp_files_path}{line}.csv',index=False,encoding='utf-8')\n",
    "\n",
    "    final = preprocessing_lines(year)\n",
    "    rm_temp_files()\n",
    "    final.to_csv(f'{main_datafile_path}{year}_{ck_week}_merged_lines.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kakao_location(place):\n",
    "    with open({key_path}) as f_:\n",
    "        kakao_key = f_.read()\n",
    "    base_url = \"https://dapi.kakao.com/v2/local/search/address.json\"\n",
    "    url = f'{base_url}?query={quote(place)}'\n",
    "    header = {'Authorization':f'KakaoAK {kakao_key}'}\n",
    "    result = requests.get(url, headers=header).json()\n",
    "    lat_ = float(result['documents'][0]['y'])\n",
    "    lng_ = float(result['documents'][0]['x'])\n",
    "    return lat_,lng_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtn_addr(df,target):\n",
    "    str_addr = df[df.지하철역 == target].도로명주소.values[-1]\n",
    "    return str_addr.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stn_lat_lng():\n",
    "    df = pd.read_excel(xlsx_path,engine='openpyxl')\n",
    "    df.to_csv(temp_info_name,index=False)\n",
    "    df = pd.read_csv(temp_info_name)\n",
    "    df = df[['역사명','역사도로명주소','운영기관명']]\n",
    "    df.rename(columns={'역사명':'지하철역','역사도로명주소':'도로명주소'},inplace=True)\n",
    "    exclude_list = ['대전교통공사', '대구도시철도공사', '부산광역시 부산교통공사', '부산-김해경전철㈜','광주광역시 도시철도공사']\n",
    "    for exclude in exclude_list:\n",
    "        df = df[df['운영기관명'] != exclude]\n",
    "    df.drop(columns=['운영기관명'],inplace=True)\n",
    "    df['지하철역'] = df['지하철역'].str.replace('(', ' ',regex=False,).str.split().str[0]\n",
    "    for i in df.index:\n",
    "        if df['지하철역'][i][-1] == '역':\n",
    "            df['지하철역'][i] = df['지하철역'][i][:-1]\n",
    "    df.drop_duplicates(subset=['지하철역'],keep='first',inplace=True)\n",
    "    df = df[~df['도로명주소'].str.contains('부산|울산')]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    temp1 =[]\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            target = df['지하철역'][i].strip()\n",
    "            temp1.append(kakao_location(rtn_addr(df,target)))\n",
    "        except:\n",
    "            print(i, df.지하철역[i])\n",
    "            \n",
    "    df_test = pd.DataFrame(temp1,columns=('lat','lng'))\n",
    "    df2 = pd.concat([df, df_test], axis=1)\n",
    "    df2.to_csv(f'{main_datafile_path}stn_r_addr_final.csv',index=False)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lat_lng(name=heatmap_data):\n",
    "    df_latlng = pd.read_csv(f'{main_datafile_path}stn_r_addr_final.csv')\n",
    "    df_latlng.drop(columns=['도로명주소'], inplace=True)\n",
    "\n",
    "    df_main = pd.read_csv(name)\n",
    "    res = pd.merge(df_main, df_latlng, on='지하철역', how='left')\n",
    "    res.to_csv(f'{main_datafile_path}lines_4heatmap_{name[:4]}.csv', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2020_주중_merged_lines.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m add_lat_lng()\n\u001b[0;32m      8\u001b[0m \u001b[39m# 일일 데이터 히트맵 처리 // 해당 년도 파일명\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m add_lat_lng(\u001b[39m'\u001b[39;49m\u001b[39m2020_주중_merged_lines.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36madd_lat_lng\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      2\u001b[0m df_latlng \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmain_datafile_path\u001b[39m}\u001b[39;00m\u001b[39mstn_r_addr_final.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_latlng\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m도로명주소\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m df_main \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(name)\n\u001b[0;32m      6\u001b[0m res \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(df_main, df_latlng, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m지하철역\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m res\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmain_datafile_path\u001b[39m}\u001b[39;00m\u001b[39mlines_4heatmap_\u001b[39m\u001b[39m{\u001b[39;00mname[:\u001b[39m4\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\YONSAI\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\YONSAI\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\YONSAI\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\YONSAI\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\YONSAI\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2020_주중_merged_lines.csv'"
     ]
    }
   ],
   "source": [
    "# 일일 데이터는 2020.csv 형식으로 'euc-kr' 인코딩 파일\n",
    "# 메인 파일 전처리\n",
    "preproc_main()\n",
    "# 일일 데이터 전처리 // 'year' , '주중 or 주말'\n",
    "preproc_sub(2020,'주중')\n",
    "# 히트맵 사용하기위한 처리\n",
    "add_lat_lng()\n",
    "# 일일 데이터 히트맵 처리 // 해당 년도 파일명\n",
    "add_lat_lng('2020_주중_merged_lines.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 주석은 내일"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
